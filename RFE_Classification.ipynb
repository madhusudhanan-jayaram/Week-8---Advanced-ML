{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3696b9c-589b-41b2-9dac-673be2f60049",
   "metadata": {},
   "source": [
    "# 📘 RFE Classification Notebook\n",
    "\n",
    "This notebook demonstrates **Recursive Feature Elimination (RFE)** for feature selection  \n",
    "and compares the performance of different **classification** models:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "- **Support Vector Machine (SVM – Linear & RBF kernels)**  \n",
    "- **K-Nearest Neighbors (KNN)**  \n",
    "- **Naive Bayes**  \n",
    "- **Decision Tree Classifier**  \n",
    "- **Random Forest Classifier**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Workflow\n",
    "1. **Load & preprocess** the dataset (train/test split, scaling).  \n",
    "2. Apply **RFE** with multiple base estimators (Logistic, SVM, Random Forest, Decision Tree) to select the top `n` features.  \n",
    "3. **Train** all classifiers on each RFE-reduced feature subset.  \n",
    "4. **Evaluate** using **Accuracy**, **Confusion Matrix**, and **Classification Report**.  \n",
    "5. **Summarize** results in a comparison **DataFrame** for quick benchmarking.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Key Insight\n",
    "- **RFE** highlights the most informative features, reducing noise and improving focus.  \n",
    "- Ensemble methods like **Random Forest** often yield higher accuracy than simpler linear models, though results depend on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "258089c0-cd08-4678-b1cf-3230dc452945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069ea2c-dc44-4a67-97fa-bd104320180c",
   "metadata": {},
   "source": [
    "### RFE Feature Selection\n",
    "\n",
    "Applies Recursive Feature Elimination (RFE) with Logistic Regression, SVM, Random Forest, and Decision Tree models to select the top n features from the dataset and returns the reduced feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11b5ec9-0d77-429e-8eb3-46e411f000b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeFeature(indep_X, dep_Y, n):\n",
    "    rfelist=[]\n",
    "    \n",
    "    log_model = LogisticRegression(solver='lbfgs')\n",
    "    RF = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "    # NB = GaussianNB()\n",
    "    DT = DecisionTreeClassifier(criterion='gini', max_features='sqrt', splitter='best', random_state=0)\n",
    "    svc_model = SVC(kernel='linear', random_state=0)\n",
    "    # knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    \n",
    "    rfemodellist=[log_model, svc_model, RF, DT] \n",
    "    for i in rfemodellist:\n",
    "        print(i)\n",
    "        log_rfe = RFE(estimator=i, n_features_to_select=n)   # ✅ fix required here\n",
    "        log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "        log_rfe_feature = log_fit.transform(indep_X)\n",
    "        rfelist.append(log_rfe_feature)\n",
    "    return rfelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b37af-59a9-415a-aefd-9794128214cb",
   "metadata": {},
   "source": [
    "### Data Split & Scaling  \n",
    "Splits the dataset into training (75%) and testing (25%), then applies StandardScaler to normalize features.  \n",
    "Returns scaled X_train, X_test along with y_train and y_test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f67e442-9b4a-4b49-a178-fe13df13c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.25, random_state = 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(indep_X,dep_Y, test_size = 0.25, random_state = 0)\n",
    "        \n",
    "        #Feature Scaling\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73a9bb-174e-435e-b3ec-6f4e3e7cc532",
   "metadata": {},
   "source": [
    "### Confusion Matrix Prediction\n",
    "\n",
    "A helper function that evaluates a classifier by predicting on X_test, comparing with y_test, and returning the accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30654f76-cabc-490f-b1a1-7c77e5f98d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_prediction(classifier,X_test):\n",
    "     y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Making the Confusion Matrix\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     from sklearn.metrics import accuracy_score \n",
    "     from sklearn.metrics import classification_report \n",
    "        #from sklearn.metrics import confusion_matrix\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     Accuracy=accuracy_score(y_test, y_pred )\n",
    "        \n",
    "     report=classification_report(y_test, y_pred)\n",
    "     return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b58035-a194-4e31-bedd-2d46b25dce5a",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "A helper function that trains a Logistic Regression model on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3805600-6406-4111-8659-68118531b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ddf1d-9801-4dff-896b-f494aacbbac3",
   "metadata": {},
   "source": [
    "### SVM (Linear Kernel)\n",
    "\n",
    "A helper function that trains an SVM classifier with a linear kernel on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd74844-3d28-47f1-9bab-a03c6d9e96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e195d-14ac-4cac-89ce-28f6c44bbf04",
   "metadata": {},
   "source": [
    "### SVM (Non-Linear RBF Kernel)\n",
    "\n",
    "A helper function that trains an SVM classifier with an RBF kernel on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40316176-b74f-4418-a6b8-4a6df5f8cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_NL(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4bd5f-d068-40e9-95a7-a7cb4ef440c3",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "A helper function that trains a Gaussian Naive Bayes classifier on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6c480c-5458-458a-a91e-001268fd8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def Navie(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79eec3-aceb-4280-974a-a274f2b7026a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "A helper function that trains a KNN classifier with k=5 using the Minkowski distance metric on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e30706c-a54d-4ef3-8a66-a1d1f3f5761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def knn(X_train,y_train,X_test):\n",
    "           \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb347-951f-459e-9532-51f9fb25e073",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "A helper function that trains a Decision Tree classifier using the entropy criterion on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bddd54-6d38-406c-9462-b6f89e8c1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223378a-399f-4ba9-9190-b785809ae5d6",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "A helper function that trains a Random Forest classifier with 10 estimators using the entropy criterion on X_train and y_train, then evaluates it on X_test using cm_prediction to return accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca417793-7996-466a-b8d1-0048fe3bd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d1789-a3ac-42f4-a0a2-4fdc9de0fbd8",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE) Classification Results\n",
    "\n",
    "A helper function that aggregates accuracy scores from different classifiers (Logistic, SVM Linear, SVM Non-Linear, KNN, Naive Bayes, Decision Tree, and Random Forest) into a DataFrame for comparison across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095c45fe-818c-4622-996f-d97dc4691ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf): \n",
    "    \n",
    "    rfedataframe=pd.DataFrame(index=['Logistic','SVC','Random','DecisionTree'],columns=['Logistic','SVMl','SVMnl',\n",
    "                                                                                        'KNN','Navie','Decision','Random'])\n",
    "\n",
    "    for number,idex in enumerate(rfedataframe.index):\n",
    "        \n",
    "        rfedataframe['Logistic'][idex]=acclog[number]       \n",
    "        rfedataframe['SVMl'][idex]=accsvml[number]\n",
    "        rfedataframe['SVMnl'][idex]=accsvmnl[number]\n",
    "        rfedataframe['KNN'][idex]=accknn[number]\n",
    "        rfedataframe['Navie'][idex]=accnav[number]\n",
    "        rfedataframe['Decision'][idex]=accdes[number]\n",
    "        rfedataframe['Random'][idex]=accrf[number]\n",
    "    return rfedataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c08a64-7018-4c8a-bd1e-592cd6bb8187",
   "metadata": {},
   "source": [
    "### Reading and Preprocessing the Dataset\n",
    "\n",
    "- Load data from `prep.csv`  \n",
    "- Create another variable `df2`  \n",
    "- Convert categorical columns into dummy variables (0/1) using `pd.get_dummies()`  \n",
    "- Use `drop_first=True` to avoid duplicate category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63fc2e89-fcc5-4cc3-9520-5ede2dd30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=pd.read_csv(\"prep.csv\",index_col=None)\n",
    "df2=dataset1\n",
    "df2 = pd.get_dummies(df2, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6ff48-595a-436f-a50d-e76ecc894215",
   "metadata": {},
   "source": [
    "### Splitting Features and Target\n",
    "\n",
    "- `indep_X` → all independent features (X values)  \n",
    "- `dep_Y` → the dependent/target variable (y value)  \n",
    "- We drop the column `classification_yes` from features,  \n",
    "  and keep it separately as the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e58706b-d820-41bf-b847-fc24de3f4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_X = df2.drop('classification_yes', axis=1)  \n",
    "dep_Y   = df2['classification_yes']            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ad98b-8635-4cf2-a367-148a9528da32",
   "metadata": {},
   "source": [
    "### RFE Feature Selection & Accuracy Lists\n",
    "\n",
    "The rfeFeature function selects the top 3 features from indep_X with respect to dep_Y, while the empty lists (acclog, accsvml, accsvmnl, accknn, accnav, accdes, accrf) are initialized to store accuracy scores for different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c40179d4-d2c4-44be-b440-2a30c1cbc135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mukil\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=0)\n",
      "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)\n",
      "DecisionTreeClassifier(max_features='sqrt', random_state=0)\n"
     ]
    }
   ],
   "source": [
    "rfelist = rfeFeature(indep_X, dep_Y, 3)\n",
    "\n",
    "acclog=[]\n",
    "accsvml=[]\n",
    "accsvmnl=[]\n",
    "accknn=[]\n",
    "accnav=[]\n",
    "accdes=[]\n",
    "accrf=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e804745-4cff-442a-9d0c-eebdbe8b68ea",
   "metadata": {},
   "source": [
    "### Model Evaluation over RFE Subsets\n",
    "\n",
    "Iterates over each RFE-reduced feature set, splits/scales the data, trains seven classifiers (Logistic, SVM-linear/RBF, KNN, Naive Bayes, Decision Tree, Random Forest), and appends their accuracies to the respective lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d97f890e-bfd4-4142-ac90-25870a84e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_Y)   \n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train,y_train,X_test)\n",
    "    acclog.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=svm_linear(X_train,y_train,X_test)  \n",
    "    accsvml.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=svm_NL(X_train,y_train,X_test)  \n",
    "    accsvmnl.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=knn(X_train,y_train,X_test)  \n",
    "    accknn.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Navie(X_train,y_train,X_test)  \n",
    "    accnav.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train,y_train,X_test)  \n",
    "    accdes.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train,y_train,X_test)  \n",
    "    accrf.append(Accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b82f18-56fb-4e81-af9e-7b41b007cb78",
   "metadata": {},
   "source": [
    "### RFE Classification Result\n",
    "\n",
    "Calls rfe_classification with the collected accuracy lists from different classifiers and returns a DataFrame summarizing their performance for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a451b3e7-7de4-4c5c-84e3-20ee69bf9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['Logistic'][idex]=acclog[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['SVMl'][idex]=accsvml[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:10: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['SVMnl'][idex]=accsvmnl[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['KNN'][idex]=accknn[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['Navie'][idex]=accnav[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:13: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['Decision'][idex]=accdes[number]\n",
      "C:\\Users\\mukil\\AppData\\Local\\Temp\\ipykernel_2012\\2386145136.py:14: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  rfedataframe['Random'][idex]=accrf[number]\n"
     ]
    }
   ],
   "source": [
    "result=rfe_classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c1486ad-fdea-45ac-8e40-406c18e6b761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVMl</th>\n",
       "      <th>SVMnl</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Navie</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Logistic  SVMl SVMnl   KNN Navie Decision Random\n",
       "Logistic         0.94  0.94  0.94  0.94  0.94     0.94   0.94\n",
       "SVC              0.87  0.87  0.87  0.87  0.87     0.87   0.87\n",
       "Random           0.91  0.92  0.93  0.93  0.86     0.91   0.94\n",
       "DecisionTree     0.93  0.93  0.94  0.95  0.74     0.95   0.97"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d92237-f561-41f4-9232-6da743890ef9",
   "metadata": {},
   "source": [
    "### 🔎 Feature Selection & Model Performance  \n",
    "\n",
    "**Feature Selection**  \n",
    "We used **Recursive Feature Elimination (RFE)** to pick the **top 3 features**.  \n",
    "RFE means:  \n",
    "- Train a model,  \n",
    "- Check which features are most important,  \n",
    "- Remove the least important ones step by step,  \n",
    "- Finally keep only the best features.  \n",
    "\n",
    "This way, the models can focus only on the useful data and ignore the noise.  \n",
    "\n",
    "---\n",
    "\n",
    "**Model Performance (Accuracy Scores)**  \n",
    "\n",
    "| RFE Base Model       | Accuracy Range |\n",
    "|----------------------|----------------|\n",
    "| Logistic Regression  | ~0.94 (same for all classifiers) |\n",
    "| SVC (Linear)         | ~0.87 (lowest overall) |\n",
    "| Random Forest        | ~0.86 – 0.94 |\n",
    "| Decision Tree        | ~0.74 – **0.97** |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Observations**  \n",
    "- Logistic Regression features → always gave around **0.94** accuracy.  \n",
    "- SVC (linear) features → weakest, stuck at **0.87**.  \n",
    "- Random Forest features → strong, went up to **0.94**.  \n",
    "- Decision Tree features → 🏆 best, reached **0.97** with Random Forest.  \n",
    "- Naive Bayes sometimes dropped low (like **0.74**) on Decision Tree features.  \n",
    "\n",
    "---\n",
    "\n",
    "🎯 **Conclusion**  \n",
    "- RFE helped us reduce the dataset to just 3 good features.  \n",
    "- The **best combo** was Decision Tree (for feature selection) + Random Forest (for classification).  \n",
    "- Simple models like Logistic and SVC worked okay, but not as powerful as trees.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
